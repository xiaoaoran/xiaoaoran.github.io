
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Aoran XIAO's Homepage</title>
  
  <meta name="author" content="Aoran Xiao">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/VIL.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Aoran.jpg", target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Aoran.jpg" class="hoverZoomLink"></a>
            </td>
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:left; display: flex; align-items: center;" >
                <name>Aoran XIAO (肖傲然)</name>
              </p>
              <p style="line-height: 1.8em;">​Postdocral Researcher
                <br>
                Geoinformatics Team
                <br>
                RIKEN Center for Advanced Intelligence Project, Japan
                <br>
                Email: aoran.xiao [at] riken.jp | <strike>aoran.xiao [at] ntu.edu.sg</strike>
              </p>
              <p style="text-align:left;">
                <span class="icon" style="margin: 0.8%;"><a href="https://scholar.google.com/citations?user=yGKsEpAAAAAJ&hl", target="_blank"><img src="images/icons/scholar.png" alt="google scholar"></a></span>
                <span class="icon" style="margin: 0.8%;"><a href="https://github.com/xiaoaoran", target="_blank"><img src="images/icons/github.png" alt="gitjub"></a></span>

              </p>
            </td>

          </tr>
        </tbody></table>

        <heading style="font-weight: bold;">Short Bio</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:20px;width:100%;vertical-align:middle">
              <p>
                ​I am currently a Postdoctoral Researcher in the <a href="https://geoinformatics2018.com">Geoinformatics Team</a> at <a href="https://www.riken.jp/en/research/labs/aip/">RIKEN AIP</a>, Japan, under the leadership of Professor <a href="https://naotoyokoya.com">Naoto Yokoya</a>. My research focuses on data-efficient learning, 2D/3D computer vision, and the efficient fine-tuning of large foundation models for multimodal data. Prior to joining RIKEN, I earned my PhD from the <a href="https://www.ntu.edu.sg/computing">College of Computing and Data Science</a> at Nanyang Technological University, Singapore, under the supervision of Professor <a href="https://personal.ntu.edu.sg/shijian.lu/">Shijian Lu</a>. I obtained my Master’s degree from <a href="http://www.lmars.whu.edu.cn/en/">LIESMARS</a>, Wuhan University, where I was co-advised by Professors <a href="http://www.lmars.whu.edu.cn/prof_web/prof_lideren/index.htm">Deren Li</a> and <a href="http://castf.org/pages/chenruizhi/chenruizhi-intro.html">Ruizhi Chen</a>, and my Bachelor’s degree from the <a href="https://rsgis.whu.edu.cn">School of Remote Sensing and Information Engineering</a>, Wuhan University.
              </p>
              <p>
                I’m open to exploring collaborations and engaging in discussions. Feel free to reach out to me!
              </p>
              <p>
                <font color="red"> Call for papers for CVPR 2025 Workshops: <a href="https://sites.google.com/view/pixfoundation/"><u>Pixel-level Vision Foundation Models</u></a>.</font>
              </p>
            </td>
          </tr>
        </tbody></table>

        <br>
        <heading style="font-weight: bold;">News</heading>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding-left:20px;width:100%;vertical-align:middle">
              <p>
                [2025-04] I am invited to serve as NeurIPS 2025 Area Chair.<br>
                [2025-01] I am co-organizing workshop on <a href="https://sites.google.com/view/pixfoundation/">PixFoundation</a> in CVPR 2025.<br>
                [2024-10] I’ve joined RIKEN AIP. <br>
                [2024-07] Our CAT-SAM is accepted to ECCV2024 as oral presentation. <br>
                [2024-06] Our survey for label-efficient learning of 3D point clouds is accepted to TPAMI!
              </p>
            </td>
          </tr>
        </tbody></table>

        <br>
        <heading style="font-weight: bold;">Selected Publications</heading>
        <table class="paper-container"><tbody>
          <br>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/RSFMs.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Foundation Models for Remote Sensing and Earth Observation: A Survey</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Weihao Xuan, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu, Naoto Yokoya
              <br>
              <em>arXiv, 2024.</em>
              <br>
              [<a href="https://arxiv.org/abs/2410.16602",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/awesome-RSFMs",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/MM-SAM-Pipeline.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Segment Anything with Multiple Modalities</papertitle>
              <br>
              <strong>Aoran Xiao*</strong>, Weihao Xuan*, Heli Qi, Yun Xing, Naoto Yokoya, Shijian Lu (*Equal contribution)
              <br>
              <em>arXiv, 2024.</em>
              <br>
              [<a href="https://arxiv.org/pdf/2408.09085",  target="_blank">paper</a>] [<a href="https://xiaoaoran.github.io/projects/MM-SAM",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/CAT-SAM.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model</papertitle>
              <br>
              <strong>Aoran Xiao*</strong>, Weihao Xuan*, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Ling Shao, Shijian Lu (*Equal contribution)
              <br>
              <em>European Conference on Computer Vision (<b>ECCV</b>), 2024. (<b>Oral paper</b>)</em>
              <br>
              [<a href="https://arxiv.org/pdf/2402.03631.pdf",  target="_blank">paper</a>] [<a href="https://xiaoaoran.github.io/projects/CAT-SAM",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/label_efficient.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>A Survey of Label-Efficient Deep Learning for 3D Point Clouds</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Xiaoqin Zhang, Ling Shao, Shijian Lu.
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2024.</em>
              <br>
              [<a href="https://arxiv.org/pdf/2305.19812.pdf",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/3D_label_efficient_learning",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/SCT.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Domain Adaptive LiDAR Point Cloud Segmentation With 3D Spatial Consistency</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Dayan Guan, Xiaoqin Zhang, Shijian Lu
              <br>
              <em>IEEE Transactions on Multimedia (<b>T-MM</b>), 2024.</em>
              <br>
              [<a href="pub/Domain_Adaptive_LiDAR_Point_Cloud_Segmentation_With_3D_Spatial_Consistency.pdf",  target="_blank">paper</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/CoCu.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Rewrite Caption Semantics: Bridging Semantic Gaps for Language-Supervised Semantic Segmentation</papertitle>
              <br>
              Yun Xing, Jian Kang, <strong>Aoran Xiao</strong>, Jiahao Nie, Ling Shao, Shijian Lu
              <br>
              <em>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2023.</em>
              <br>
              [<a href="https://arxiv.org/pdf/2309.13505.pdf",  target="_blank">paper</a>] [<a href="https://github.com/xing0047/CoCu",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/SemanticSTF.gif" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing.
              <br>
              <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (<b>CVPR</b>), 2023.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_3D_Semantic_Segmentation_in_the_Wild_Learning_Generalized_Models_for_CVPR_2023_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/SemanticSTF",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/point_ssl_survey.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Unsupervised Representation Learning for Point Clouds with Deep Neural Networks: A Survey</papertitle>
              <br>
              <strong>Aoran Xiao*</strong>, Jiaxing Huang*, Dayan Guan, Xiaoqin Zhang, Shijian Lu (*Equal contribution).
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>T-PAMI</b>), 2023.</em>
              <br>
              [<a href="https://arxiv.org/pdf/2202.13589.pdf",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/3d_url_survey",  target="_blank">project</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/polarmix.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>PolarMix: A General Data Augmentation Technique for LiDAR Point Clouds</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Jiaxing Huang, Dayan Guan, Kaiwen Cui, Shijian Lu, Ling Shao.
              <br>
              <em>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2022.</em>
              <br>
              [<a href="https://arxiv.org/abs/2208.00223",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/polarmix",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/caco.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Category Contrast for Unsupervised Domain Adaptation in Visual Tasks</papertitle>
              <br>
              Jiaxing Huang, Dayan Guan, <strong>Aoran Xiao</strong>, Shijian Lu, Ling Shao.
              <br>
              <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (<b>CVPR</b>), 2022.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Huang_Category_Contrast_for_Unsupervised_Domain_Adaptation_in_Visual_Tasks_CVPR_2022_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/jxhuang0508/CaCo",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/Unbiased.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation</papertitle>
              <br>
              Dayan Guan, Jiaxing Huang, <strong>Aoran Xiao</strong>, Shijian Lu, Ling Shao.
              <br>
              <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (<b>CVPR</b>), 2022.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guan_Unbiased_Subclass_Regularization_for_Semi-Supervised_Semantic_Segmentation_CVPR_2022_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/Dayan-Guan/USRN",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/point_transfer.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Jiaxing Huang, Dayan Guan, Fangneng Zhan, Shijian Lu.
              <br>
              <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2022.</em>
              <br>
              [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/20183/19942",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/SynLiDAR",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/hcl.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data</papertitle>
              <br>
              Jiaxing Huang, Dayan Guan, <strong>Aoran Xiao</strong>, Shijian Lu
              <br>
              <em>Advances in Neural Information Processing Systems (<b>NeurIPS</b>), 2021.</em>
              <br>
              [<a href="https://proceedings.neurips.cc/paper/2021/file/1dba5eed8838571e1c80af145184e515-Paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/jxhuang0508/HCL",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/rda.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>RDA: Robust Domain Adaptation via Fourier Adversarial Attacking</papertitle>
              <br>
              Jiaxing Huang, Dayan Guan, <strong>Aoran Xiao</strong>, Shijian Lu
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>), 2021.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_RDA_Robust_Domain_Adaptation_via_Fourier_Adversarial_Attacking_ICCV_2021_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/jxhuang0508/RDA",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/davideo.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Domain Adaptive Video Segmentation via Temporal Consistency Regularization</papertitle>
              <br>
              Dayan Guan, Jiaxing Huang, <strong>Aoran Xiao</strong>, Shijian Lu
              <br>
              <em>International Conference on Computer Vision (<b>ICCV</b>), 2021.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Guan_Domain_Adaptive_Video_Segmentation_via_Temporal_Consistency_Regularization_ICCV_2021_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/Dayan-Guan/DA-VSN",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/cvrn.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Cross-View Regularization for Domain Adaptive Panoptic Segmentation</papertitle>
              <br>
              Jiaxing Huang, Dayan Guan, <strong>Aoran Xiao</strong>, Shijian Lu, Ling Shao.
              <br>
              <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (<b>CVPR</b>), 2021.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_Cross-View_Regularization_for_Domain_Adaptive_Panoptic_Segmentation_CVPR_2021_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/jxhuang0508/CVRN",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/fsdr.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>FSDR: Frequency Space Domain Randomization for Domain Generalization</papertitle>
              <br>
              Jiaxing Huang, Dayan Guan, <strong>Aoran Xiao</strong>, Shijian Lu, Ling Shao.
              <br>
              <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (<b>CVPR</b>), 2021.</em>
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Huang_FSDR_Frequency_Space_Domain_Randomization_for_Domain_Generalization_CVPR_2021_paper.pdf",  target="_blank">paper</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/fpsnet.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>FPS-Net: A convolutional fusion network for large-scale LiDAR point cloud segmentation</papertitle>
              <br>
              <strong>Aoran Xiao</strong>, Xiaofei Yang, Shijian Lu, Dayan Guan, Jiaxing Huang.
              <br>
              <em>ISPRS journal of Photogrammetry and Remote Sensing, 2021.</em>
              <br>
              [<a href="https://arxiv.org/pdf/2103.00738",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/FPS-Net",  target="_blank">project</a>]
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="images/papers/undet.png" alt="clean-usnob" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection</papertitle>
              <br>
              Dayan Guan, Jiaxing Huang, <strong>Aoran Xiao</strong>, Shijian Lu, Yanpeng Cao.
              <br>
              <em>IEEE Transactions on Multimedia (<b>T-MM</b>), 2021.</em>
              <br>
              [<a href="https://arxiv.org/abs/2103.00236",  target="_blank">paper</a>] [<a href="https://github.com/Dayan-Guan/UaDAN",  target="_blank">project</a>]
          </tr>
        </tbody></table>

        <br><br><br>
        <heading style="font-weight: bold;">Released Datasets</heading>
        <table class="paper-container"><tbody>
          <br>
          <tr>
            <td class="paper-item-img">
              <img src="images/papers/synlidar.gif" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>SynLiDAR: A Large-Scale Synthetic LiDAR Point Cloud Dataset with Point-Wise Annotations</papertitle>
              <br>
              SynLiDAR is a large-scale synthetic LiDAR sequential point cloud dataset with point-wise annotations. 13 sequences of LiDAR point cloud with around 20k scans (over 19 billion points and 32 semantic classes) are collected from virtual urban cities, suburban towns, neighborhood, and harbor.
              <br>
              [<a href="https://arxiv.org/abs/2107.05399",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/SynLiDAR",  target="_blank">project</a>] [<a href="https://docs.google.com/forms/d/e/1FAIpQLScZR3re0YFn59mlnag8s7vD5p4JaMkX2oxug5rn1K5bc5C-4g/viewform?pli=1",  target="_blank">download</a>]
            </p>
            </td>
          </tr>

          <tr>
            <td class="paper-item-img">
              <img src="./images/papers/SemanticSTF.gif" width="200" height="100">
            </td>
            <td width="75%" valign="middle">
              <p style="line-height: 1.8em;">
              <papertitle>SemanticSTF: An Adverse-Weather LiDAR Point Cloud Dataset</papertitle>
              <br>
              SemanticSTF is a large-scale adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3D semantic segmentation under various adverse weather conditions.
              <br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xiao_3D_Semantic_Segmentation_in_the_Wild_Learning_Generalized_Models_for_CVPR_2023_paper.pdf",  target="_blank">paper</a>] [<a href="https://github.com/xiaoaoran/SemanticSTF",  target="_blank">project</a>] [<a href="https://docs.google.com/forms/d/e/1FAIpQLSeChw4k27lUUspcLeD4af1C_HtPxhsCCCZTcU8QxpTsDvdDng/viewform",  target="_blank">download</a>]
            </p>
            </td>
          </tr>
        </tbody></table>

        <br><br><br>
        <heading style="font-weight: bold;">Academic Service</heading>
        <br><br>
        <table class="news" width="100%" align="center" border="0" cellpadding="0"><tbody>
          <tr>
            <td width="100%" valign="center">
              <subheading style="font-weight: bold;">Workshops & Tutorials</subheading>
              <ul style="padding-left: 6%; line-height: 1.5em;">
                <li>Organizer, CVPR 2025 Workshop: <a href="https://sites.google.com/view/pixfoundation/"><u>Pixel-level Vision Foundation Models</u></a>. </li>
              </ul>
              <subheading style="font-weight: bold;">Conference Reviews</subheading>
              <ul style="padding-left: 6%; line-height: 1.5em;">
                <li>International Conference on Computer Vision (ICCV) 2023.</li>
                <li>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2025,2024,2023,2022.</li>
                <li>Conference and Workshop on Neural Information Processing Systems (NeurIPS) 2024,2023.</li>
                <li>European Conference on Computer Vision (ECCV) 2024, 2022.</li>
                <li>International Conference on Learning Representations (ICLR) 2025,2024.</li>
                <li>International Conference on Machine Learning (ICML) 2025,2024.</li>
                <li>AAAI Conference on Artificial Intelligence (AAAI) 2025,2024.</li>
              </ul>
              <subheading style="font-weight: bold;">Journal Reviews</subheading>
              <ul style="height: auto;">
                <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).</li>
                <li>IEEE Transactions on Image Processing (TIP).</li>
                <li>IEEE Transactions on Intelligent Vehicles (TIV).</li>
                <li>IEEE Transactions on Multimedia (TMM).</li>
                <li>International Journal of Computer Vision (IJCV).</li>
                <li>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).</li>
                <li>ISPRS Journal of Photogrammetry and Remote Sensing.</li>
                <li>Pattern Recognition.</li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=BeXrlI9ts-nayEItO_gDSrwVCCEIuU-hnwtZy-zYVbg'></script>

        <footer>
          <p style="text-align:center"> © XIAO Aoran | Last updated: Jan 2025</p>
          <p style="text-align:center;font-size:small;">
            Design and source code from <a style="font-size:small;" href="https://jonbarron.info", target="_blank">Jon Barron's website</a>.
          </a></p>
        </footer>


      </td>
    </tr>
  </table>
</body>

</html>